{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import math\n",
    "import csv\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler,TensorDataset\n",
    "from torch import optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "\n",
    "from flair.embeddings import BertEmbeddings, FlairEmbeddings, StackedEmbeddings \n",
    "from flair.data import Sentence\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescue_code(function):\n",
    "    import inspect\n",
    "    get_ipython().set_next_input(\"\".join(inspect.getsourcelines(function)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV data\n",
    "\n",
    "train = pd.read_csv(\"../data/train.csv\")\n",
    "test = pd.read_csv(\"../data/test.csv\")\n",
    "train.drop('Unnamed: 0',inplace=True, axis=1)\n",
    "test.drop('Unnamed: 0',inplace=True, axis=1)\n",
    "\n",
    "# Retain the rights columns\n",
    "\n",
    "X_train = train.drop(['id','answer_start', 'answer_stop','text', 'is_impossible'], axis=1)\n",
    "X_test = test.drop(['id','answer_start', 'answer_stop','text', 'is_impossible'], axis=1)\n",
    "\n",
    "y_train = train.text\n",
    "y_test = test.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the Sentence start and end markers\n",
    "\n",
    "def separators(string):\n",
    "    string = \"[CLS] {} [EOS]\".format(string)\n",
    "    return string\n",
    "\n",
    "X_train['context'] = X_train.context.apply(separators)\n",
    "X_train['question'] = X_train.question.apply(separators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the tokens for the input\n",
    "\n",
    "TOKENS_COMPUTED = True\n",
    "\n",
    "if not TOKENS_COMPUTED:\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "    tok_context_train = [tokenizer.tokenize(con) for con in X_train.context]\n",
    "    tok_context_test = [tokenizer.tokenize(con) for con in X_test.context]\n",
    "\n",
    "    tok_qs_train = [tokenizer.tokenize(qs) for qs in X_train.question]\n",
    "    tok_qs_test = [tokenizer.tokenize(qs) for qs in X_test.question]\n",
    "\n",
    "    tok_answer_train = [tokenizer.tokenize(con) for con in y_train.astype(str)]\n",
    "    tok_answer_test = [tokenizer.tokenize(con) for con in y_test.astype(str)]\n",
    "\n",
    "\n",
    "# Load the tokens\n",
    "\n",
    "if TOKENS_COMPUTED:\n",
    "    with open('../data/train/train_context_tok.json') as f:\n",
    "        tok_context_train = json.load(f)\n",
    "\n",
    "    with open('../data/train/train_qs_tok.json') as f:\n",
    "        tok_qs_train = json.load(f)\n",
    "\n",
    "    with open('../data/train/train_answer_tok.json') as f:\n",
    "        tok_answer_train = json.load(f)\n",
    "\n",
    "    ## Get json dicts for test\n",
    "\n",
    "    with open('../data/test/test_context_tok.json') as f:\n",
    "        tok_context_test = json.load(f)\n",
    "\n",
    "    with open('../data/test/test_qs_tok.json') as f:\n",
    "        tok_qs_test = json.load(f)\n",
    "\n",
    "    with open('../data/test/test_answer_tok.json') as f:\n",
    "        tok_answer_test = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vocab class containing dictionaries and ids to words translators\n",
    "# Any word that has less than 100 occurences won't be added to the vocabulary\n",
    "THRES = 100\n",
    "\n",
    "class Vocab:\n",
    "    def __init__(self, str_lst):\n",
    "        self.str_lst = str_lst\n",
    "        self.vocab, self.len_vocab = self.get_vocab()\n",
    "        self.reversed_vocab = self.reverse_dict()\n",
    "        self.ids_col = self.str_to_int(False)\n",
    "        self.words_col = self.int_to_str(False)\n",
    "\n",
    "    def get_vocab(self):\n",
    "        vocab = [item for sublist in self.str_lst for item in sublist]\n",
    "        vocab_dict = {}\n",
    "        counter = 1\n",
    "        for token in vocab:\n",
    "            if token not in vocab_dict.keys():\n",
    "                vocab_dict[token] = {'id':counter, 'occurences':1}\n",
    "                counter += 1\n",
    "            else:\n",
    "                vocab_dict[token]['occurences'] += 1\n",
    "\n",
    "        length = [1 if vocab_dict[tok]['occurences'] > THRES else 0 for tok in vocab_dict.keys()]\n",
    "\n",
    "        final_dict = {}\n",
    "        counter = 1\n",
    "        for iter in range(len(length)):\n",
    "            if length[iter]:\n",
    "                this = list(vocab_dict.keys())[iter]\n",
    "                final_dict[this] = counter\n",
    "                counter +=1\n",
    "\n",
    "        final_dict['UNK'] = 0\n",
    "        return final_dict, sum(length)\n",
    "\n",
    "    def str_to_int(self, other):\n",
    "        ints = []\n",
    "        if other is False:\n",
    "            for sentence in self.str_lst:\n",
    "                sent = []\n",
    "                for word in sentence:\n",
    "                    try:\n",
    "                        inti = self.vocab[word]\n",
    "                    except KeyError:\n",
    "                        inti = 0\n",
    "                    sent.append(inti)\n",
    "                ints.append(sent)\n",
    "            return pd.Series(ints)\n",
    "        else:\n",
    "            for sentence in other:\n",
    "                sent = []\n",
    "                for word in sentence:\n",
    "                    try:\n",
    "                        inti = self.vocab[word]\n",
    "                    except KeyError:\n",
    "                        inti = 0\n",
    "                    sent.append(inti)\n",
    "                ints.append(sent)\n",
    "            return pd.Series(ints)\n",
    "\n",
    "    def reverse_dict(self):\n",
    "        return {v: k for k, v in self.vocab.items()}\n",
    "\n",
    "    def int_to_str(self, other):\n",
    "        words = []\n",
    "        if other is False:\n",
    "            for sentence in self.ids_col:\n",
    "                words.append([self.reversed_vocab[ids] for ids in sentence])\n",
    "        else:\n",
    "            for sentence in other:\n",
    "                words.append([self.reversed_vocab[ids] for ids in sentence])\n",
    "            \n",
    "        return pd.Series(words)\n",
    "\n",
    "class Squad:\n",
    "    def __init__(self, context, qs):\n",
    "        self.context = Vocab(context)\n",
    "        self.qs = self.context.str_to_int(qs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building vocabulary...\n",
      "Vocab built\n"
     ]
    }
   ],
   "source": [
    "print('Building vocabulary...')\n",
    "\n",
    "X_train_vocab = Squad(tok_context_train, tok_qs_train)\n",
    "X_test_vocab = Squad(tok_context_test, tok_qs_test)\n",
    "\n",
    "X_train[\"ids_context\"] = X_train_vocab.context.ids_col\n",
    "X_train[\"ids_qs\"] = X_train_vocab.qs\n",
    "y_train = X_train_vocab.context.str_to_int(tok_answer_train)\n",
    "\n",
    "X_test[\"ids_context\"] = X_test_vocab.context.ids_col\n",
    "X_test[\"ids_qs\"] = X_test_vocab.qs\n",
    "y_test = X_test_vocab.context.str_to_int(tok_answer_test)\n",
    "\n",
    "print('Vocab built')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenating ids\n"
     ]
    }
   ],
   "source": [
    "# Join ids for input prep\n",
    "\n",
    "print('Concatenating ids')\n",
    "def concat(df , col_a, col_b, name):\n",
    "    new_col = df[col_a] + df[col_b]\n",
    "    df[name] = new_col\n",
    "\n",
    "concat(X_train, 'ids_context', 'ids_qs', 'ids')\n",
    "concat(X_test, 'ids_context', 'ids_qs', 'ids')\n",
    "\n",
    "X_train['ids'] = X_train.ids.apply(np.array)\n",
    "X_test['ids'] = X_test.ids.apply(np.array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Dataset Sentences\n",
      "Computing tokens for further BERT embedding\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the input with Flair function\n",
    "\n",
    "print('Building Dataset Sentences')\n",
    "\n",
    "concat(X_train, 'question', 'context', 'txt')\n",
    "concat(X_test, 'question', 'context', 'txt')\n",
    "\n",
    "\n",
    "print('Computing tokens for further BERT embedding')\n",
    "\n",
    "X_train['txt'] = X_train.txt.apply(Sentence)\n",
    "X_test['txt'] = X_test.txt.apply(Sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "669\n",
      "68\n"
     ]
    }
   ],
   "source": [
    "# Get sizes for input and outputs\n",
    "\n",
    "sizes_in = [len(i) for i in X_train.txt]\n",
    "print(max(sizes_in))\n",
    "\n",
    "sizes_out = [len(y_train[i]) for i in range(len(y_train))]\n",
    "print(max(sizes_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Vocab size is: 14514\n",
      "Deleting rows with inputs too large for embedding\n"
     ]
    }
   ],
   "source": [
    "# Delete rows with input exceeding max length\n",
    "\n",
    "train_vocab_size = X_train_vocab.context.len_vocab+1\n",
    "\n",
    "print('Train Vocab size is: {}'.format(train_vocab_size))\n",
    "print('Deleting rows with inputs too large for embedding')\n",
    "\n",
    "X_train['target'] = y_train\n",
    "X_train = X_train[X_train['txt'].map(len) <= 512]\n",
    "\n",
    "X_test['target'] = y_test\n",
    "X_test = X_test[X_test['txt'].map(len) <= 512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tokens\n",
    "\n",
    "SAVE_VOCAB = False\n",
    "\n",
    "if SAVE_VOCAB:\n",
    "    ## Save vocabs and tokens for train\n",
    "    with open('../data/train/train_qs_tok.json', 'w') as fp:\n",
    "        json.dump(tok_qs_train, fp)\n",
    "\n",
    "    with open('../data/train/train_context_vocab.json', 'w') as fp:\n",
    "        json.dump(X_train_vocab.context.vocab, fp)\n",
    "\n",
    "    with open('../data/train/train_context_tok.json', 'w') as fp:\n",
    "        json.dump(tok_context_train, fp)\n",
    "\n",
    "    with open('../data/train/train_answer_tok.json', 'w') as fp:\n",
    "        json.dump(tok_answer_train, fp)\n",
    "    \n",
    "    ## Save vocabs and tokens for test\n",
    "\n",
    "    with open('../data/test/test_qs_tok.json', 'w') as fp:\n",
    "        json.dump(tok_qs_test, fp)\n",
    "\n",
    "    with open('../data/test/test_context_vocab.json', 'w') as fp:\n",
    "        json.dump(X_test_vocab.context.vocab, fp)\n",
    "\n",
    "    with open('../data/test/test_context_tok.json', 'w') as fp:\n",
    "        json.dump(tok_context_test, fp)\n",
    "\n",
    "    with open('../data/test/test_answer_tok.json', 'w') as fp:\n",
    "        json.dump(tok_answer_test, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n",
      "68\n"
     ]
    }
   ],
   "source": [
    "sizes_in = [len(i) for i in X_train.txt]\n",
    "print(max(sizes_in))\n",
    "\n",
    "sizes_out = [len(y_train[i]) for i in range(len(y_train))]\n",
    "print(max(sizes_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oversized  = [1 if size>512 else 0 for size in sizes_in]\n",
    "sum(oversized)/len(sizes_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make all outputs the same size\n",
    "\n",
    "def padding(series, max_size):\n",
    "    return [np.concatenate((sentence, np.zeros(max_size-len(sentence)))) for sentence in series]\n",
    "\n",
    "target_tensor = padding(X_train.target, max(sizes_out))\n",
    "X_train['target'] = target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init Flair embeddings\n",
    "flair_forward_embedding = FlairEmbeddings('multi-forward')\n",
    "flair_backward_embedding = FlairEmbeddings('multi-backward')\n",
    "\n",
    "# init multilingual BERT\n",
    "bert_embedding = BertEmbeddings('bert-base-multilingual-cased')\n",
    "\n",
    "stacked_embeddings = StackedEmbeddings(\n",
    "    embeddings=[flair_forward_embedding, flair_backward_embedding, bert_embedding])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use embedding on a sentence\n",
    "\n",
    "def embedding(sentence):\n",
    "    stacked_embeddings.embed(sentence)\n",
    "    return torch.Tensor([list(word.embedding) for word in sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainset = torch.utils.data.TensorDataset(in_train, out_train)\n",
    "#trainloader = torch.utils.data.DataLoader(trainset, batch_size=1, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 31892658 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "\n",
    "model.apply(init_weights)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"The model has {} trainable parameters\".format(count_parameters(model)))\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_train(model, optimizer, criterion, clip, dataframe):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for sto_batch in range(dataframe.shape[0]):\n",
    "        \n",
    "        src = dataframe['txt'][sto_batch]\n",
    "        trg = dataframe['target'][sto_batch]\n",
    "        \n",
    "        print('Got the batch')\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        print('Launching BERT embedding')\n",
    "        src = embedding(src).unsqueeze_(0)\n",
    "        trg = torch.LongTensor(trg)\n",
    "        \n",
    "        print('Launching LSTM forward')\n",
    "        output = model(src, trg)\n",
    "        output = output.squeeze_(0)\n",
    "        \n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        print('Loss is: ',loss.item())\n",
    "        print('Launching Backprop')\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            src = batch[0]\n",
    "            trg = batch[1]\n",
    "\n",
    "            output = model(src, trg, 0) #turn off teacher forcing\n",
    "\n",
    "            output = output[1:].view(-1, output.shape[-1])\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got the batch\n",
      "Launching BERT embedding\n",
      "Launching LSTM forward\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Variable' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-173-ce900884b90e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstochastic_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;31m#valid_loss = evaluate(model, valid_iterator, criterion)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-171-5e0ed09bf9fa>\u001b[0m in \u001b[0;36mstochastic_train\u001b[0;34m(model, optimizer, criterion, clip, dataframe)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Launching LSTM forward'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/studies/classes/nlp/venv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-169-f0c0c5187da1>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, trg, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mmax_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mvocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mencoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Variable' is not defined"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 1\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = stochastic_train(model, optimizer, criterion, CLIP, X_train)\n",
    "    #valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    #if valid_loss < best_valid_loss:\n",
    "    #    best_valid_loss = valid_loss\n",
    "    #    torch.save(model.state_dict(), '../models/tut1-model.pt')\n",
    "    \n",
    "    print('Epoch: {} | Time: {}m {}s'.format(epoch+1, epoch_mins, epoch_secs))\n",
    "    print('\\nTrain Loss: {} | Train PPL: {}'.format(train_loss, math.exp(train_loss)))\n",
    "    #print('\\n Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}'.format())\n",
    "    \n",
    "    \n",
    "#model.load_state_dict(torch.load('../models/tut1-model.pt'))\n",
    "\n",
    "#test_loss = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "#print('| Test Loss: {} | Test PPL: {} |'.format(test_loss, math.exp(test_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
