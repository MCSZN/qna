{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLPy.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpJwSvxFUHhd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "c7def368-545a-4b58-f2d9-48085d059ad7"
      },
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQXMB_J3UlqY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = \"drive/My Drive/MSc BD & BA/Period 3/NLPy/data.json\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-TkUW9yfl1Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 install flair"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOE3AHkDVQK-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import time\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "from pprint import pprint\n",
        "from flair.data import Sentence\n",
        "from flair.embeddings import BertEmbeddings\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sT8TQ7LCVtfC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(path, 'r') as f:\n",
        "    data = json.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6Sq78hBWEf0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Utils\n",
        "\n",
        "def batch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs\n",
        "\n",
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "def make_input(dictionnary, key):\n",
        "    line = dictionnary[key]\n",
        "    \n",
        "    title = '[SOT]{}[EOT]'.format(line['titre'])\n",
        "    message =  '[SOS]{}[EOS]'.format(line['message'])\n",
        "    \n",
        "    ans = int(line['num_answers'])\n",
        "    cat = int(line['category'])\n",
        "    member = int(line['member'])\n",
        "    solved = int(line['is_solved'])\n",
        "    state = int(line['state'])\n",
        "    topic = int(line['topic_id'])\n",
        "    visits = int(line['visits'])\n",
        "    votes = int(line['votes'])\n",
        "    \n",
        "    text = title+message\n",
        "    meta_data = [ans,cat,member,solved,state,topic,visits,votes]\n",
        "    \n",
        "    return text, meta_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fu1194WNWIgS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "4b3a9bcd-0652-4c85-d6df-9977f4f60571"
      },
      "source": [
        "# init multilingual BERT\n",
        "bert_embedding = BertEmbeddings('bert-base-multilingual-cased')\n",
        "\n",
        "\n",
        "def embedding(sentence):\n",
        "    bert_embedding.embed(sentence)\n",
        "    return torch.Tensor([list(word.embedding) for word in sentence])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-05-22 22:08:25,268 The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 995526/995526 [00:00<00:00, 8808521.51B/s]\n",
            "100%|██████████| 662804195/662804195 [00:15<00:00, 44093537.33B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Jf2ZDaNiOHb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim, layers , label_size, \n",
        "                 batch_size, use_gpu):\n",
        "        super(LSTMClassifier, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.batch_size = batch_size\n",
        "        self.use_gpu = use_gpu\n",
        "        self.layers = layers\n",
        "\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, layers)\n",
        "        self.out = nn.Linear(hidden_dim, label_size)\n",
        "        self.hidden = self.init_hidden()\n",
        "\n",
        "    def init_hidden(self):\n",
        "        if self.use_gpu:\n",
        "            h0 = Variable(torch.zeros(self.layers, self.batch_size, self.hidden_dim).cuda())\n",
        "            c0 = Variable(torch.zeros(self.layers, self.batch_size, self.hidden_dim).cuda())\n",
        "        else:\n",
        "            h0 = Variable(torch.zeros(self.layers, self.batch_size, self.hidden_dim))\n",
        "            c0 = Variable(torch.zeros(self.layers, self.batch_size, self.hidden_dim))\n",
        "        return (h0, c0)\n",
        "\n",
        "    def forward(self, sentence):\n",
        "        embeds = sentence\n",
        "        x = embeds.view(len(sentence), self.batch_size, -1)\n",
        "        lstm_out, self.hidden = self.lstm(x, self.hidden)\n",
        "        y = self.out(lstm_out[-1])\n",
        "        return y\n",
        "    \n",
        "    \n",
        "class HybridDense(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, label_size):\n",
        "        super(HybridDense, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.label_size = label_size\n",
        "        \n",
        "        self.input = nn.Linear(input_dim, hidden_dim)\n",
        "        self.hidden = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.out = nn.Linear(hidden_dim, label_size)\n",
        "        \n",
        "    def forward(self, meta_data):\n",
        "        x = self.input(meta_data)\n",
        "        x = self.hidden(x)\n",
        "        x = self.hidden(x)\n",
        "        x = self.hidden(x)\n",
        "        y = self.out(x)\n",
        "        return y\n",
        "    \n",
        "class HybridModel(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim, layers , label_size, \n",
        "                 batch_size, use_gpu, input_dim, hidden_dim2): # LSTM => DENSE\n",
        "        super(HybridModel,self).__init__()\n",
        "        \n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.layers = layers\n",
        "        self.label_size = label_size\n",
        "        self.batch_size = batch_size\n",
        "        self.use_gpu = use_gpu\n",
        "        \n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim2 = hidden_dim2\n",
        "        \n",
        "        self.lstm_model = LSTMClassifier(\n",
        "            embedding_dim=self.embedding_dim,\n",
        "            hidden_dim=self.hidden_dim,\n",
        "            layers=self.layers,\n",
        "            label_size=self.label_size,\n",
        "            batch_size=self.batch_size,\n",
        "            use_gpu=self.use_gpu\n",
        "            ).to(device)\n",
        "        \n",
        "        self.dense = HybridDense(\n",
        "            input_dim = self.input_dim, \n",
        "            hidden_dim = self.hidden_dim2,\n",
        "            label_size = self.label_size\n",
        "            ).to(device)\n",
        "        \n",
        "        self.final_layer = nn.Linear(2, label_size)\n",
        "        \n",
        "    def forward(self, sentence, meta_data):\n",
        "        lstm_out = self.lstm_model(sentence).squeeze_(0)\n",
        "        dense_out = self.dense(meta_data)\n",
        "        out = torch.cat((lstm_out,dense_out), 0)\n",
        "        y = self.final_layer(out)\n",
        "        return y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rNIkXgE_RuT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We first count the probability of getting a 1 as a target\n",
        "count = 0\n",
        "total = len(list(data.keys()))\n",
        "for key, val in data.items():\n",
        "    try:\n",
        "        if int(val['is_best_answer']) == 1:\n",
        "            count += 1\n",
        "    except:\n",
        "        continue\n",
        "PART = count/total\n",
        "\n",
        "# We then compute the threshold we need to approximatily have 1/2 of the time\n",
        "# a target that is 1 so as to balance the training\n",
        "prob = 1-(PART/4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFVU_f2JgqMf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def stochastic_train(model, optimizer, criterion, clip, data_source):\n",
        "    \n",
        "    losses = {}\n",
        "    iter_count = 0\n",
        "    mod_count = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for key in data_source.keys():\n",
        "        iter_count +=1\n",
        "        \n",
        "        start_time = time.time()\n",
        "        try:\n",
        "            sentence, meta_data = make_input(data_source, key)\n",
        "            trg = int(data_source[key]['is_best_answer'])\n",
        "        except:\n",
        "            continue\n",
        "        \n",
        "        # Got the minibatch\n",
        "        optimizer.zero_grad()\n",
        "        if (trg == 0) and (np.random.rand() < prob):\n",
        "            continue\n",
        "        if len(sentence) > 512:\n",
        "            continue\n",
        "        # Launching BERT embedding\n",
        "        sentence = embedding(Sentence(sentence)).to(device)\n",
        "        \n",
        "        meta_data = torch.Tensor(meta_data).to(device)\n",
        "        trg = torch.Tensor([trg]).to(device)\n",
        "        \n",
        "        # Launching model forward\n",
        "        output = model(sentence, meta_data)\n",
        "        \n",
        "        model.zero_grad()\n",
        "        \n",
        "        \n",
        "        loss = criterion(torch.sigmoid(output), trg)\n",
        "        loss.backward(retain_graph=True)\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        end_time = time.time()\n",
        "        batch_mins, batch_secs = batch_time(start_time, end_time)\n",
        "        \n",
        "        print('Time: {}:{}\\nLoss: {}'.format(batch_mins,batch_secs,loss.item()))\n",
        "        losses[key] = loss.item()\n",
        "        \n",
        "        \n",
        "        if iter_count% 300 == 0:\n",
        "            with open(\"drive/My Drive/MSc BD & BA/Period 3/NLPy/results.json\", 'w') as f:\n",
        "                json.dump(losses, f)\n",
        "            torch.save(model.state_dict(), \n",
        "                       'drive/My Drive/MSc BD & BA/Period 3/NLPy/model{}.pt'.format(\n",
        "                       mod_count))\n",
        "            mod_count +=1\n",
        "        \n",
        "    return losses, mod_count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaVdr3Z7hIlR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def execution(\n",
        "        env=\"drive\",\n",
        "        path=\"data.json\",\n",
        "        use_gpu=True,\n",
        "        train=True,\n",
        "        load=False,\n",
        "        predict=False,\n",
        "        model_path='',\n",
        "        sentence='',\n",
        "        model_id='',\n",
        "        meta_data=[0]*8,\n",
        "        **kwargs):\n",
        "\n",
        "    if env==\"drive\":\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "        path = \"drive/My Drive/MSc BD & BA/Period 3/NLPy/data.json\"\n",
        "    elif env==\"local\":\n",
        "        path=\"data/teach_data/data.json\"\n",
        "    \n",
        "    with open(path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = HybridModel(\n",
        "            embedding_dim=3072,\n",
        "            hidden_dim=128,\n",
        "            layers=3,\n",
        "            label_size=1,\n",
        "            batch_size=1,\n",
        "            use_gpu=use_gpu,\n",
        "            input_dim=8,\n",
        "            hidden_dim2=7\n",
        "            ).to(device)\n",
        "    \n",
        "\n",
        "    model.apply(init_weights)\n",
        "    print(\"The model has {} trainable parameters\".format(count_parameters(model\n",
        "                                                                         )))\n",
        "    optimizer = optim.Adam(model.parameters())\n",
        "    criterion = nn.BCELoss()\n",
        "    clip = 1\n",
        "\n",
        "    if train:\n",
        "        train_loss, mod_count = stochastic_train(model, optimizer, criterion, \n",
        "                                                 clip, data)\n",
        "\n",
        "    if load:\n",
        "        model.load_state_dict(torch.load(model_path+'model{}.pt'.format(\n",
        "            model_id)))\n",
        "\n",
        "    if predict:\n",
        "        with torch.no_grad():\n",
        "            out = model(embedding(Sentence(sentence)),torch.Tensor(meta_data))\n",
        "            out = torch.sigmoid(out)\n",
        "        print(out)\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    # This file is optimized for Google Colab execution\n",
        "    execution()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mqGywVeLYLJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
